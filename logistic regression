from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

#y = df['Class']
#X = df.iloc[:,:-1]

# 对特征数据进行标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 对目标数据进行编码
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

from sklearn.metrics import classification_report

# 在测试集上进行预测
y_pred = model.predict(X_test)

# 生成分类报告
print("分类报告：")
print(classification_report(y_test, y_pred, target_names=encoder.classes_))

带有混淆图，及决策边界的代码
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

plt.rcParams["font.family"] = ["SimHei"]
plt.rcParams['axes.unicode_minus'] = False

# 数据准备（保持原有代码）
y = df['Class']
X = df.iloc[:,:-1]

# 对特征数据进行标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 对目标数据进行编码
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)

# 训练模型（保持原有代码）
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 在测试集上进行预测（保持原有代码）
y_pred = model.predict(X_test)

# 生成分类报告（保持原有代码）
print("测试集分类报告：")
print(classification_report(y_test, y_pred, target_names=encoder.classes_))

# 计算混淆矩阵（保持原有代码）
cm = confusion_matrix(y_test, y_pred)

# 可视化混淆矩阵（保持原有代码）
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=encoder.classes_,
            yticklabels=encoder.classes_)
plt.xlabel('预测标签')
plt.ylabel('真实标签')
plt.title('测试集混淆矩阵')
plt.show()

# 3. 可视化决策边界（需要降维到二维）
pca = PCA(n_components=2)
X_train_2d = pca.fit_transform(X_train)
X_test_2d = pca.transform(X_test)

# 在降维数据上训练新模型
model_2d = LogisticRegression(max_iter=1000)
model_2d.fit(X_train_2d, y_train)

# 创建网格以绘制决策边界
h = 0.02  # 网格步长
x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1
y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# 预测网格点的类别
Z = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 绘制决策边界
plt.figure(figsize=(12, 8))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)

# 绘制训练数据点
plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, 
            cmap=plt.cm.coolwarm, edgecolors='k', s=50, label='训练数据')
plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_test, 
            cmap=plt.cm.coolwarm, marker='x', s=50, label='测试数据')

plt.colorbar(label='类别')
plt.legend()
plt.xlabel('主成分1')
plt.ylabel('主成分2')
plt.title('逻辑回归模型的决策边界')
plt.tight_layout()
plt.show()

# 4. 评估训练集、测试集和完整数据集的预测结果
def evaluate_dataset(X, y, dataset_name, model, encoder):
    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)
    cm = confusion_matrix(y, y_pred)
    
    print(f"\n{dataset_name}评估结果:")
    print(f"准确率: {accuracy:.4f}")
    print(classification_report(y, y_pred, target_names=encoder.classes_))
    
    # 可视化混淆矩阵
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=encoder.classes_,
                yticklabels=encoder.classes_)
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(f'{dataset_name}混淆矩阵')
    plt.tight_layout()
    plt.show()
    
    return accuracy, cm

# 评估训练集
train_accuracy, train_cm = evaluate_dataset(X_train, y_train, "训练集", model, encoder)

# 评估测试集
test_accuracy, test_cm = evaluate_dataset(X_test, y_test, "测试集", model, encoder)

# 评估完整数据集
all_accuracy, all_cm = evaluate_dataset(X_scaled, y_encoded, "完整数据集", model, encoder)
